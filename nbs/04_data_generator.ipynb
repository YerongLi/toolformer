{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generator\n",
    "\n",
    "> Fill in a module description hered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp data_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/education/DATA/projects/ai/toolformer/env/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "from typing import List, Callable, Tuple\n",
    "\n",
    "import torch\n",
    "from einops import rearrange\n",
    "from torchtyping import TensorType\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "from toolformer.api import BaseAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DataGenerator:\n",
    "    def __init__(self, config: dict, model: Callable, tokenizer: Callable, apis: List[BaseAPI],):\n",
    "        start_character = config[\"data_generator\"][\"api_start_character\"]\n",
    "        end_character = config[\"data_generator\"][\"api_end_character\"]\n",
    "        output_character = config[\"data_generator\"][\"api_output_character\"]\n",
    "        \n",
    "        # add a space, because when the model generate a token, it's also include a \"space\"\n",
    "        self.api_start_token = tokenizer(f' {start_character}', return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "        self.api_end_token = tokenizer(f'{end_character}', return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "        self.api_output_character = tokenizer(f' {output_character}', return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "        \n",
    "        self.top_k = config[\"data_generator\"][\"top_k\"]\n",
    "        self.sampling_threshold = config[\"data_generator\"][\"sampling_threshold\"]\n",
    "        self.filtering_threshold = config[\"data_generator\"][\"filtering_threshold\"]\n",
    "        \n",
    "        self.apis = apis\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        # TODO: handle for cases that the sentence contains \".\\n\\n\"\n",
    "        self.eos_token_id = tokenizer(\".\\n\\n\")[\"input_ids\"][0]\n",
    "    \n",
    "    def _sampling(self, logits: TensorType[\"batch_size\", \"seq_len\", \"vocab_size\"]):\n",
    "        pass\n",
    "    \n",
    "    def _generate_api_position(\n",
    "        self,\n",
    "        prompt_ids: TensorType[\"batch_size\", \"seq_len\"], # the ids of the prompt\n",
    "    ) -> Tuple[\n",
    "        TensorType[\"batch_size\", \"n_positions\"], # The positions of api call\n",
    "        TensorType[\"batch_size\", \"seq_len\"] # The generated text\n",
    "    ]:\n",
    "        # TODO: add support batch\n",
    "        generated_ids = prompt_ids\n",
    "        api_positions = torch.tensor([])\n",
    "        \n",
    "        with torch.no_grad():    \n",
    "            while True:\n",
    "                logits = self.model(\n",
    "                    input_ids=generated_ids.unsqueeze(0),\n",
    "                ).logits\n",
    "\n",
    "                last_logit = logits[0, -1, :]\n",
    "                probs = torch.softmax(last_logit, dim=-1)\n",
    "                \n",
    "                # find the top k tokens for api call\n",
    "                top_k_tokens = torch.topk(probs, k=5, dim=-1).indices\n",
    "                \n",
    "                if self.api_start_token in top_k_tokens:\n",
    "                    api_position = torch.tensor([len(generated_ids)]) # the current idx\n",
    "                    api_positions = torch.cat((api_positions, api_position), dim=0)\n",
    "                \n",
    "                # sampling a token\n",
    "                # next_token = torch.multinomial(probs, num_samples=1)\n",
    "                next_token = torch.argmax(probs, dim=-1)\n",
    "                next_token = next_token.unsqueeze(0)\n",
    "                generated_ids = torch.cat([generated_ids, next_token], dim=0)\n",
    "                \n",
    "                print(\"--------------------\")\n",
    "                print(f\"next_token={next_token}\")\n",
    "                print(f\"positions={api_positions}\")\n",
    "                print(f\"text={self.tokenizer.decode(generated_ids)}\")\n",
    "                \n",
    "                if next_token == self.eos_token_id: break\n",
    "        \n",
    "        return api_positions, generated_ids\n",
    "    \n",
    "    def _sampling_api(\n",
    "        self,\n",
    "        positions: TensorType[\"batch_size\", \"n_positions\"],\n",
    "        generated_ids: TensorType[\"batch_size\", \"seq_len\"],\n",
    "        prompt: PromptTemplate\n",
    "    ):\n",
    "        for position in positions:\n",
    "            for api in self.apis:\n",
    "                condition_text = generated_ids[:position]\n",
    "                conditioned_prompt = prompt.format(input=condition_text)\n",
    "                pass\n",
    "    \n",
    "    def _filter_api(\n",
    "        self,\n",
    "        idxs: List[int]\n",
    "    ):\n",
    "        pass\n",
    "    \n",
    "    def generate(\n",
    "        self,\n",
    "        prompt_tempalte: PromptTemplate,\n",
    "        text: str,\n",
    "    ) -> List[str]:\n",
    "        prompt = prompt_tempalte.format(input=text)\n",
    "        prompt_ids = self.tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"][0]  \n",
    "        \n",
    "        # sampling\n",
    "        # TODO: add support batch\n",
    "        positions, generated_ids = self._generate_api_position(prompt_ids)\n",
    "        return positions, generated_ids\n",
    "        # filtering\n",
    "        \n",
    "        # return"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
