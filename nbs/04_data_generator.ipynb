{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generator\n",
    "\n",
    "> Fill in a module description hered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp data_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from typing import List, Callable, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchtyping import TensorType\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "from toolformer.api import BaseAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DataGenerator:\n",
    "    def __init__(self, config: dict, model: Callable, tokenizer: Callable, apis: List[BaseAPI],):\n",
    "        start_character = config[\"data_generator\"][\"api_start_character\"]\n",
    "        end_character = config[\"data_generator\"][\"api_end_character\"]\n",
    "        output_character = config[\"data_generator\"][\"api_output_character\"]\n",
    "        \n",
    "        # add a space, because when the model generate a token, it's also include a \"space\"\n",
    "        self.api_start_token = tokenizer(f' {start_character}', return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "        self.api_end_token = tokenizer(end_character, return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "        self.api_output_character = tokenizer(f' {output_character}', return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "        \n",
    "        self.top_k = config[\"data_generator\"][\"top_k\"]\n",
    "        self.sampling_threshold = config[\"data_generator\"][\"sampling_threshold\"]\n",
    "        self.filtering_threshold = config[\"data_generator\"][\"filtering_threshold\"]\n",
    "        \n",
    "        self.apis = apis\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        # TODO: handle for cases that the sentence contains \".\\n\\n\"\n",
    "        self.eos_token_id = tokenizer(\".\\n\\n\")[\"input_ids\"][0]\n",
    "    \n",
    "    def _sample_api_position(\n",
    "        self,\n",
    "        prompt_ids: TensorType[\"batch_size\", \"seq_len\"], # the ids of the prompt\n",
    "    ) -> Tuple[\n",
    "        TensorType[\"batch_size\", \"n_positions\"], # The positions of api call\n",
    "        TensorType[\"batch_size\", \"seq_len\"] # The generated text\n",
    "    ]:\n",
    "        # TODO: add support batch\n",
    "        generated_ids = prompt_ids\n",
    "        api_positions = torch.tensor([])\n",
    "        \n",
    "        with torch.no_grad():    \n",
    "            while True:\n",
    "                logits = self.model(\n",
    "                    input_ids=generated_ids.unsqueeze(0),\n",
    "                ).logits\n",
    "\n",
    "                last_logit = logits[0, -1, :]\n",
    "                probs = torch.softmax(last_logit, dim=-1)\n",
    "                \n",
    "                # find the top k tokens for api call\n",
    "                # TODO: add filter by larger than sampling_threshold\n",
    "                top_k_tokens = torch.topk(probs, k=5, dim=-1)\n",
    "                \n",
    "                if self.api_start_token in top_k_tokens.indices:\n",
    "                    api_position = torch.tensor([len(generated_ids)]) # the current idx\n",
    "                    api_positions = torch.cat((api_positions, api_position), dim=0)\n",
    "                \n",
    "                # sampling a token\n",
    "                # next_token = torch.multinomial(probs, num_samples=1)\n",
    "                next_token = torch.argmax(probs, dim=-1)\n",
    "                next_token = next_token.unsqueeze(0)\n",
    "                generated_ids = torch.cat([generated_ids, next_token], dim=0)\n",
    "                \n",
    "                # print(\"--------------------\")\n",
    "                # print(f\"next_token={next_token}\")\n",
    "                # print(f\"positions={api_positions}\")\n",
    "                # print(f\"text={self.tokenizer.decode(generated_ids)}\")\n",
    "                \n",
    "                if next_token == self.eos_token_id: break\n",
    "        \n",
    "        return api_positions.long(), generated_ids.long()\n",
    "\n",
    "    def _obtain_api_call(\n",
    "        self,\n",
    "        positions: TensorType[\"batch_size\", \"n_positions\"],\n",
    "        generated_ids: TensorType[\"batch_size\", \"seq_len\"]\n",
    "    ) -> TensorType[\"batch_size\", \"n_positions\", \"seq_len\"]:\n",
    "        MAX_PAD = 1000\n",
    "        PAD_TOKEN = self.tokenizer.pad_token_id\n",
    "        candidates = torch.tensor([])\n",
    "\n",
    "        for position in positions:\n",
    "            text_ids = torch.cat([generated_ids[:position], self.api_start_token])\n",
    "            output = self.model.generate(\n",
    "                input_ids=text_ids.unsqueeze(0),\n",
    "                eos_token_id=self.eos_token_id,\n",
    "                max_new_tokens=100,\n",
    "            )\n",
    "            \n",
    "            N_PAD = MAX_PAD - output.shape[-1]\n",
    "            candidates = torch.cat([\n",
    "                candidates,\n",
    "                F.pad(output[0], pad=(0, N_PAD), value=PAD_TOKEN).unsqueeze(0).long()\n",
    "            ], dim=0)\n",
    "        return candidates.long()\n",
    "    \n",
    "    def _sampling_api(\n",
    "        self,\n",
    "        positions: TensorType[\"batch_size\", \"n_positions\"],\n",
    "        generated_ids: TensorType[\"batch_size\", \"seq_len\"],\n",
    "        prompt: PromptTemplate\n",
    "    ):\n",
    "        for position in positions:\n",
    "            for api in self.apis:\n",
    "                condition_text = generated_ids[:position]\n",
    "                conditioned_prompt = prompt.format(input=condition_text)\n",
    "                pass\n",
    "    \n",
    "    def _filter_api(\n",
    "        self,\n",
    "        idxs: List[int]\n",
    "    ):\n",
    "        pass\n",
    "    \n",
    "    def generate(\n",
    "        self,\n",
    "        prompt_tempalte: PromptTemplate,\n",
    "        text: str,\n",
    "    ) -> List[str]:\n",
    "        prompt = prompt_tempalte.format(input=text)\n",
    "        prompt_ids = self.tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"][0]  \n",
    "        # TODO: add support batch\n",
    "        # sampling\n",
    "        api_positions, generated_ids = self._sample_api_position(prompt_ids)\n",
    "        \n",
    "        # obtaining API calls\n",
    "        candidates = self._obtain_api_call(api_positions, generated_ids)\n",
    "        # filtering\n",
    "        \n",
    "        return candidates"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
