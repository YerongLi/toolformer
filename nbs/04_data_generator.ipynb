{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generator\n",
    "\n",
    "> Fill in a module description hered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp data_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/education/DATA/projects/ai/toolformer/env/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import re\n",
    "from typing import List, Callable, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchtyping import TensorType\n",
    "from einops import rearrange\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "from toolformer.api import BaseAPI\n",
    "from toolformer.api import CalculatorAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DataGenerator:\n",
    "    def __init__(self, config: dict, model: Callable, tokenizer: Callable, apis: List[BaseAPI],):\n",
    "        start_character = config[\"data_generator\"][\"api_start_character\"]\n",
    "        end_character = config[\"data_generator\"][\"api_end_character\"]\n",
    "        output_character = config[\"data_generator\"][\"api_output_character\"]\n",
    "        \n",
    "        # add a space, because when the model generate a token, it's also include a \"space\"\n",
    "        self.api_start_token = tokenizer(f' {start_character}', return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "        self.api_end_token = tokenizer(end_character, return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "        self.api_output_token = tokenizer(f'{output_character}', return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "        \n",
    "        self.top_k = config[\"data_generator\"][\"top_k\"]\n",
    "        self.sampling_threshold = config[\"data_generator\"][\"sampling_threshold\"]\n",
    "        self.filtering_threshold = config[\"data_generator\"][\"filtering_threshold\"]\n",
    "        \n",
    "        self.apis = apis\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        # TODO: handle for cases that the sentence contains \".\\n\\n\"\n",
    "        self.eos_token_id = tokenizer(\".\\n\\n\")[\"input_ids\"][0]\n",
    "    \n",
    "    def extract_api_request_content(self, text: str, api_name: str) -> str:\n",
    "        start_tag = f\"{api_name}(\"\n",
    "        end_tag = \")\"\n",
    "        start_idx = text.find(start_tag)\n",
    "        if start_idx == -1:\n",
    "            return None\n",
    "        start_idx += len(start_tag)\n",
    "        end_idx = text.find(end_tag, start_idx)\n",
    "        if end_idx == -1:\n",
    "            return None\n",
    "        return text[start_idx:end_idx]\n",
    "    \n",
    "    def extract_api_syntax(self, sentence: str, api_name: str) -> str:\n",
    "        pattern = r\"\\[{}\\(.*?\\)\\]\".format(api_name)\n",
    "        matches = re.findall(pattern, sentence)\n",
    "        return matches\n",
    "    \n",
    "    def sample_api_position(\n",
    "        self,\n",
    "        prompt_ids: TensorType[\"batch_size\", \"seq_len\"], # the ids of the prompt\n",
    "    ) -> Tuple[\n",
    "        TensorType[\"batch_size\", \"n_positions\"], # The positions of api call\n",
    "        TensorType[\"batch_size\", \"seq_len\"] # The generated text\n",
    "    ]:\n",
    "        # TODO: add support batch\n",
    "        \n",
    "        # the ids of the prompt and generated_ids\n",
    "        prompt_and_generated_ids = prompt_ids\n",
    "        # only the ids of the generated_ids\n",
    "        generated_ids = torch.tensor([])\n",
    "        api_positions = torch.tensor([])\n",
    "        i = torch.tensor([0])\n",
    "        \n",
    "        with torch.no_grad():    \n",
    "            while True:\n",
    "                logits = self.model(\n",
    "                    input_ids=prompt_and_generated_ids.unsqueeze(0),\n",
    "                ).logits\n",
    "\n",
    "                last_logit = logits[0, -1, :]\n",
    "                probs = torch.softmax(last_logit, dim=-1)\n",
    "                \n",
    "                # find the top k tokens for api call\n",
    "                # TODO: add filter by larger than sampling_threshold\n",
    "                top_k_tokens = torch.topk(probs, k=5, dim=-1)\n",
    "                \n",
    "                if self.api_start_token in top_k_tokens.indices:\n",
    "                    # api_position = torch.tensor([len(generated_ids)]) # the current idx\n",
    "                    api_positions = torch.cat((api_positions, i), dim=0)\n",
    "                \n",
    "                # sampling a token\n",
    "                # next_token = torch.multinomial(probs, num_samples=1)\n",
    "                next_token = torch.argmax(probs, dim=-1)\n",
    "                next_token = next_token.unsqueeze(0)\n",
    "                \n",
    "                prompt_and_generated_ids = torch.cat([prompt_and_generated_ids, next_token], dim=0)\n",
    "                generated_ids = torch.cat([generated_ids, next_token], dim=0)\n",
    "                \n",
    "                if next_token == self.eos_token_id:\n",
    "                    break\n",
    "                else:\n",
    "                    i += 1\n",
    "        \n",
    "        return api_positions.long(), generated_ids.long()\n",
    "\n",
    "    def obtain_api_response(\n",
    "        self,\n",
    "        prompt_ids: TensorType[\"batch_size\", \"seq_len\"],\n",
    "        positions: TensorType[\"batch_size\", \"n_positions\"],\n",
    "        generated_ids: TensorType[\"batch_size\", \"seq_len\"]\n",
    "    ) -> TensorType[\"batch_size\", \"n_positions\", \"seq_len\"]:\n",
    "        \n",
    "        MAX_PAD = 50\n",
    "        PAD_TOKEN = self.tokenizer.pad_token_id\n",
    "        \n",
    "        # the ids before the start of an api call\n",
    "        pre_api_ids = torch.tensor([])\n",
    "\n",
    "        for position in positions:\n",
    "            text_ids = torch.cat([generated_ids[:position], self.api_start_token], dim=0)\n",
    "            padded_text_ids = F.pad(text_ids, pad=(MAX_PAD - text_ids.shape[-1], 0), value=PAD_TOKEN)\n",
    "            \n",
    "            pre_api_ids = torch.cat([\n",
    "                pre_api_ids,\n",
    "                rearrange(padded_text_ids, \"... -> 1 ...\")\n",
    "            ])\n",
    "        \n",
    "        PROMPT_LENGTH = len(prompt_ids)\n",
    "        \n",
    "        # TODO: optimzie this\n",
    "        prompt_and_pre_api_ids = torch.tensor([])\n",
    "        for x in pre_api_ids:\n",
    "            prompt_and_pre_api_ids = torch.cat([\n",
    "                prompt_and_pre_api_ids,\n",
    "                torch.cat([prompt_ids, x]).unsqueeze(0)\n",
    "            ], dim=0)\n",
    "                     \n",
    "        with torch.no_grad():\n",
    "            candidates = self.model.generate(\n",
    "                input_ids=prompt_and_pre_api_ids.long(),\n",
    "                eos_token_id=self.eos_token_id,\n",
    "                max_new_tokens=50,\n",
    "            )\n",
    "        \n",
    "        # filter out the prompt template\n",
    "        # only keep the generated ids\n",
    "        candidates = candidates[:, PROMPT_LENGTH:]\n",
    "        \n",
    "        return candidates\n",
    "\n",
    "    # def extract_pred_from_candidate(\n",
    "    #     candidate_ids: TensorType[\"seq_len\"] # the initial prompting to guide the model + generated_ids\n",
    "    # ) -> TensorType[\"pred_len\"]:\n",
    "    #     \"\"\"Extract the generated ids from the [prompt + generated_ids].\n",
    "        \n",
    "    #     Example: Your task is to add two numbers. [PREDICTION].\n",
    "    #     Extracted ids: [PREDICTION].\n",
    "\n",
    "    #     Returns:\n",
    "    #         torch.Tensor: The ids of the prediction\n",
    "    #     \"\"\"\n",
    "    #     pred_ids = candidate_ids[PROMPT_LENGTH:]\n",
    "    #     return pred_ids\n",
    "    \n",
    "    def _generate_conditioning_prompts(\n",
    "        self,\n",
    "        prompt_ids: TensorType[\"batch_size\", \"seq_len\"],\n",
    "        candidate_ids: TensorType[\"batch_size\", \"n_candidates\", \"seq_len\"],\n",
    "    ):\n",
    "        calculator_api = CalculatorAPI()\n",
    "        # conditioning_prompts = torch.tensor([])\n",
    "        conditioning_api_ids = torch.tensor([])\n",
    "        target_ids = torch.tensor([])\n",
    "\n",
    "        SPACE_TOKEN = self.tokenizer(\" .\", return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "        API_NAME = \"Calculator\"\n",
    "        MAX_PAD = 100\n",
    "        PAD_TOKEN = self.tokenizer.pad_token_id\n",
    "\n",
    "        for text_ids in candidate_ids:\n",
    "            # the ids of the prediction\n",
    "            text = self.tokenizer.decode(text_ids, skip_special_tokens=True)\n",
    "            \n",
    "            api_request_content = self.extract_api_request_content(text, api_name=API_NAME)\n",
    "            api_response = calculator_api(api_request_content)\n",
    "            api_response_ids = self.tokenizer(api_response, return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "            # Format: -> [api_response]\n",
    "            api_response_with_arrow_ids = torch.cat([self.api_output_token, api_response_ids], dim=0)\n",
    "            \n",
    "            api_syntax = self.extract_api_syntax(text, api_name=API_NAME)\n",
    "            api_syntax_ids = self.tokenizer(api_syntax, return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "            api_syntax_with_response_ids = torch.cat([api_syntax_ids[:-1], api_response_with_arrow_ids, api_syntax_ids[-1:]])\n",
    "            api_syntax_without_response_ids = torch.cat([api_syntax_ids[:-1], self.api_output_token, api_syntax_ids[-1:]])\n",
    "            \n",
    "            api_start_idx = torch.where(text_ids == self.api_start_token)[0]\n",
    "            pred_exclude_api_ids = text_ids[:api_start_idx]\n",
    "            next_token_ids = text_ids[api_start_idx + 1]\n",
    "                        \n",
    "            promt_without_api = pred_exclude_api_ids\n",
    "            # prompt_with_api_and_response = torch.cat([api_syntax_with_response_ids, SPACE_TOKEN, pred_exclude_api_ids], dim=0)\n",
    "            # prompt_with_api_with_empty_response = torch.cat([api_syntax_without_response_ids, SPACE_TOKEN, pred_exclude_api_ids], dim=0)\n",
    "            \n",
    "            # padded_prompt_without_api = rearrange(\n",
    "            #     F.pad(promt_without_api, pad=(0, (MAX_PAD - promt_without_api.shape[-1])), value=PAD_TOKEN),\n",
    "            #     \"... -> 1 ...\"\n",
    "            # )\n",
    "            # padded_prompt_with_api_with_empty_response = rearrange(\n",
    "            #     F.pad(prompt_with_api_with_empty_response, pad=(0, (MAX_PAD - prompt_with_api_with_empty_response.shape[-1])), value=PAD_TOKEN),\n",
    "            #     \"... -> 1 ...\"\n",
    "            # )\n",
    "            # padded_prompt_with_api_and_response = rearrange(\n",
    "            #     F.pad(prompt_with_api_and_response, pad=(0, (MAX_PAD - prompt_with_api_and_response.shape[-1])), value=PAD_TOKEN),\n",
    "            #     \"... -> 1 ...\"\n",
    "            # )\n",
    "            \n",
    "            padded_api_without_response = rearrange(\n",
    "                F.pad(api_syntax_without_response_ids, pad=(0, (MAX_PAD - api_syntax_without_response_ids.shape[-1])), value=PAD_TOKEN),\n",
    "                \"... -> 1 ...\"\n",
    "            )\n",
    "            padded_api_with_response = rearrange(\n",
    "                F.pad(api_syntax_with_response_ids, pad=(0, (MAX_PAD - api_syntax_with_response_ids.shape[-1])), value=PAD_TOKEN),\n",
    "                \"... -> 1 ...\"\n",
    "            )\n",
    "            \n",
    "            # padded_prompt = torch.cat([\n",
    "            #     padded_prompt_without_api,\n",
    "            #     padded_prompt_with_api_with_empty_response,\n",
    "            #     padded_prompt_with_api_and_response,\n",
    "            # ], dim=0)\n",
    "            \n",
    "            padded_api_call = torch.cat([\n",
    "                padded_api_without_response,\n",
    "                padded_api_with_response\n",
    "            ], dim=0)\n",
    "            padded_api_call = rearrange(padded_api_call, \"... -> 1 ...\")\n",
    "            \n",
    "            # padded_prompt = rearrange(padded_prompt, \"... -> 1 ...\")\n",
    "\n",
    "            # conditioning_prompts = torch.cat([conditioning_prompts, padded_prompt], dim=0).long()\n",
    "            conditioning_api_ids = torch.cat([conditioning_api_ids, padded_api_call], dim=0).long()\n",
    "            target_ids = torch.cat([target_ids, torch.tensor(next_token_ids)], dim=0).long()\n",
    "                    \n",
    "        return target_ids, conditioning_api_ids\n",
    "\n",
    "    def filter_api( \n",
    "        self,\n",
    "        prompt_ids: TensorType[\"batch_size\", \"seq_len\"],\n",
    "        candiates: TensorType[\"batch_size\", \"n_positions\", \"seq_len\"]\n",
    "    ):\n",
    "        target_ids, conditioning_prompts = self._generate_conditioning_prompts(prompt_ids, candiates)\n",
    "        return target_ids, conditioning_prompts\n",
    "    \n",
    "    def generate(\n",
    "        self,\n",
    "        prompt_tempalte: PromptTemplate,\n",
    "        text: str,\n",
    "    ) -> List[str]:\n",
    "        # TODO: add support batch\n",
    "        prompt = prompt_tempalte.format(input=text)\n",
    "        prompt_ids = self.tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"][0]  \n",
    "    \n",
    "        # sampling positions\n",
    "        api_start_idx, generated_ids = self.sample_api_position(prompt_ids)\n",
    "        \n",
    "        # obtaining api responses\n",
    "        candidates = self.obtain_api_response(prompt_ids, api_start_idx, generated_ids)\n",
    "\n",
    "        # filtering\n",
    "        target_ids, conditioning_prompts = self.filter_api(prompt_ids, candidates)\n",
    "        \n",
    "        return api_start_idx, generated_ids, target_ids, conditioning_prompts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
