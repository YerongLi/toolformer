{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generator\n",
    "\n",
    "> Fill in a module description hered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp data_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/education/DATA/projects/ai/toolformer/env/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import re\n",
    "from typing import List, Callable, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchtyping import TensorType\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "from toolformer.api import BaseAPI\n",
    "from toolformer.api import CalculatorAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DataGenerator:\n",
    "    def __init__(self, config: dict, model: Callable, tokenizer: Callable, apis: List[BaseAPI],):\n",
    "        start_character = config[\"data_generator\"][\"api_start_character\"]\n",
    "        end_character = config[\"data_generator\"][\"api_end_character\"]\n",
    "        output_character = config[\"data_generator\"][\"api_output_character\"]\n",
    "        \n",
    "        # add a space, because when the model generate a token, it's also include a \"space\"\n",
    "        self.api_start_token = tokenizer(f' {start_character}', return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "        self.api_end_token = tokenizer(end_character, return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "        self.api_output_token = tokenizer(f'{output_character}', return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "        \n",
    "        self.top_k = config[\"data_generator\"][\"top_k\"]\n",
    "        self.sampling_threshold = config[\"data_generator\"][\"sampling_threshold\"]\n",
    "        self.filtering_threshold = config[\"data_generator\"][\"filtering_threshold\"]\n",
    "        \n",
    "        self.apis = apis\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        # TODO: handle for cases that the sentence contains \".\\n\\n\"\n",
    "        self.eos_token_id = tokenizer(\".\\n\\n\")[\"input_ids\"][0]\n",
    "    \n",
    "    def _sample_api_position(\n",
    "        self,\n",
    "        prompt_ids: TensorType[\"batch_size\", \"seq_len\"], # the ids of the prompt\n",
    "    ) -> Tuple[\n",
    "        TensorType[\"batch_size\", \"n_positions\"], # The positions of api call\n",
    "        TensorType[\"batch_size\", \"seq_len\"] # The generated text\n",
    "    ]:\n",
    "        # TODO: add support batch\n",
    "        generated_ids = prompt_ids\n",
    "        api_positions = torch.tensor([])\n",
    "        \n",
    "        with torch.no_grad():    \n",
    "            while True:\n",
    "                logits = self.model(\n",
    "                    input_ids=generated_ids.unsqueeze(0),\n",
    "                ).logits\n",
    "\n",
    "                last_logit = logits[0, -1, :]\n",
    "                probs = torch.softmax(last_logit, dim=-1)\n",
    "                \n",
    "                # find the top k tokens for api call\n",
    "                # TODO: add filter by larger than sampling_threshold\n",
    "                top_k_tokens = torch.topk(probs, k=5, dim=-1)\n",
    "                \n",
    "                if self.api_start_token in top_k_tokens.indices:\n",
    "                    api_position = torch.tensor([len(generated_ids)]) # the current idx\n",
    "                    api_positions = torch.cat((api_positions, api_position), dim=0)\n",
    "                \n",
    "                # sampling a token\n",
    "                # next_token = torch.multinomial(probs, num_samples=1)\n",
    "                next_token = torch.argmax(probs, dim=-1)\n",
    "                next_token = next_token.unsqueeze(0)\n",
    "                generated_ids = torch.cat([generated_ids, next_token], dim=0)\n",
    "                \n",
    "                if next_token == self.eos_token_id: break\n",
    "        \n",
    "        return api_positions.long(), generated_ids.long()\n",
    "\n",
    "    def _obtain_api_call(\n",
    "        self,\n",
    "        positions: TensorType[\"batch_size\", \"n_positions\"],\n",
    "        generated_ids: TensorType[\"batch_size\", \"seq_len\"]\n",
    "    ) -> TensorType[\"batch_size\", \"n_positions\", \"seq_len\"]:\n",
    "        MAX_PAD = 1000\n",
    "        PAD_TOKEN = self.tokenizer.pad_token_id\n",
    "        candidates = torch.tensor([])\n",
    "\n",
    "        for position in positions:\n",
    "            text_ids = torch.cat([generated_ids[:position], self.api_start_token])\n",
    "            output = self.model.generate(\n",
    "                input_ids=text_ids.unsqueeze(0),\n",
    "                eos_token_id=self.eos_token_id,\n",
    "                max_new_tokens=100,\n",
    "            )\n",
    "            \n",
    "            N_PAD = MAX_PAD - output.shape[-1]\n",
    "            candidates = torch.cat([\n",
    "                candidates,\n",
    "                F.pad(output[0], pad=(0, N_PAD), value=PAD_TOKEN).unsqueeze(0).long()\n",
    "            ], dim=0)\n",
    "        return candidates.long()\n",
    "\n",
    "    def extract_api_request_content(self, text: str, api_name: str) -> str:\n",
    "        start_tag = f\"{api_name}(\"\n",
    "        end_tag = \")\"\n",
    "        start_idx = text.find(start_tag)\n",
    "        if start_idx == -1:\n",
    "            return None\n",
    "        start_idx += len(start_tag)\n",
    "        end_idx = text.find(end_tag, start_idx)\n",
    "        if end_idx == -1:\n",
    "            return None\n",
    "        return text[start_idx:end_idx]\n",
    "    \n",
    "    def extract_api_syntax(self, sentence: str, api_name: str) -> str:\n",
    "        pattern = r\"\\[{}\\(.*?\\)\\]\".format(api_name)\n",
    "        matches = re.findall(pattern, sentence)\n",
    "        return matches\n",
    "    \n",
    "    def _sampling_api(\n",
    "        self,\n",
    "        positions: TensorType[\"batch_size\", \"n_positions\"],\n",
    "        generated_ids: TensorType[\"batch_size\", \"seq_len\"],\n",
    "        prompt: PromptTemplate\n",
    "    ):\n",
    "        pass\n",
    "    \n",
    "    def _filter_api(\n",
    "        self,\n",
    "        prompt_ids: TensorType[\"batch_size\", \"seq_len\"],\n",
    "        candidates: List[int]\n",
    "    ):\n",
    "        calculator_api = CalculatorAPI()\n",
    "        conditioning_prompts = []\n",
    "\n",
    "        PROMPT_LENGTH = len(prompt_ids)\n",
    "        SPACE_TOKEN = self.tokenizer(\" .\", return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "        API_NAME = \"Calculator\"\n",
    "\n",
    "        for candidate in candidates:\n",
    "            # the ids of the prediction\n",
    "            pred_ids = candidate[PROMPT_LENGTH:]\n",
    "            pred = self.tokenizer.decode(pred_ids, skip_special_tokens=True)\n",
    "            \n",
    "            api_request_content = self.extract_api_request_content(pred, api_name=API_NAME)\n",
    "            api_response = calculator_api(api_request_content)\n",
    "            api_response_with_arrow = torch.cat([self.api_output_token, self.tokenizer(api_response, return_tensors=\"pt\")[\"input_ids\"][0]], dim=0)\n",
    "            \n",
    "            api_syntax = self.extract_api_syntax(pred, api_name=API_NAME)\n",
    "            api_syntax_ids = self.tokenizer(api_syntax, return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "            api_syntax_with_response_ids = torch.cat([api_syntax_ids[:-1], api_response_with_arrow, api_syntax_ids[-1:]])\n",
    "            \n",
    "            api_start_idx = torch.where(pred_ids == self.api_start_token)[0]\n",
    "            pred_exclude_api_ids = pred_ids[:api_start_idx]\n",
    "            next_token_ids = pred_ids[api_start_idx + 1]\n",
    "            \n",
    "            conditioning_prompts.append(torch.cat([api_syntax_with_response_ids, SPACE_TOKEN, pred_exclude_api_ids], dim=0))\n",
    "                    \n",
    "        return conditioning_prompts\n",
    "    \n",
    "    def generate(\n",
    "        self,\n",
    "        prompt_tempalte: PromptTemplate,\n",
    "        text: str,\n",
    "    ) -> List[str]:\n",
    "        # TODO: add support batch\n",
    "        prompt = prompt_tempalte.format(input=text)\n",
    "        prompt_ids = self.tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"][0]  \n",
    "        \n",
    "        # sampling\n",
    "        api_start_positions, generated_ids = self._sample_api_position(prompt_ids)\n",
    "        \n",
    "        # obtaining API calls\n",
    "        candidates = self._obtain_api_call(api_start_positions, generated_ids)\n",
    "    \n",
    "        # filtering\n",
    "        conditioning_prompts = self._filter_api(prompt_ids, candidates)\n",
    "        \n",
    "        return conditioning_prompts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
