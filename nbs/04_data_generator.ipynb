{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generator\n",
    "\n",
    "> Fill in a module description hered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp data_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import re\n",
    "from typing import List, Callable, Tuple, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchtyping import TensorType\n",
    "from einops import rearrange\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "from toolformer.api import BaseAPI\n",
    "from toolformer.api import CalculatorAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DataGenerator:\n",
    "    def __init__(self, config: dict, model: Callable, tokenizer: Callable, apis: List[BaseAPI]):\n",
    "        start_character = config[\"data_generator\"][\"api_start_character\"]\n",
    "        end_character = config[\"data_generator\"][\"api_end_character\"]\n",
    "        output_character = config[\"data_generator\"][\"api_output_character\"]\n",
    "        \n",
    "        # add a space, because when the model generate a token, it's also include a \"space\"\n",
    "        self.api_start_token = tokenizer(f' {start_character}', return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "        self.api_end_token = tokenizer(end_character, return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "        self.api_output_token = tokenizer(f'{output_character}', return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "        \n",
    "        self.top_k = config[\"data_generator\"][\"top_k\"]\n",
    "        self.sampling_threshold = config[\"data_generator\"][\"sampling_threshold\"]\n",
    "        self.filtering_threshold = config[\"data_generator\"][\"filtering_threshold\"]\n",
    "        \n",
    "        self.apis = apis\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        # TODO: handle for cases that the sentence contains \".\\n\\n\"\n",
    "        self.pad_token_id = tokenizer.pad_token_id\n",
    "        self.eos_token_id = tokenizer(\".\\n\\n\")[\"input_ids\"][0]\n",
    "    \n",
    "    def extract_api_request_content(self, text: str, api_name: str) -> str:\n",
    "        start_tag = f\"{api_name}(\"\n",
    "        end_tag = \")\"\n",
    "        start_idx = text.find(start_tag)\n",
    "        if start_idx == -1:\n",
    "            return None\n",
    "        start_idx += len(start_tag)\n",
    "        end_idx = text.find(end_tag, start_idx)\n",
    "        if end_idx == -1:\n",
    "            return None\n",
    "        return text[start_idx:end_idx]\n",
    "    \n",
    "    def extract_api_syntax(self, sentence: str, api_name: str) -> str:\n",
    "        pattern = r\"\\[{}\\(.*?\\)\\]\".format(api_name)\n",
    "        matches = re.findall(pattern, sentence)\n",
    "        return matches\n",
    "    \n",
    "    def sample_api_position(\n",
    "        self,\n",
    "        prompt_ids: TensorType[\"batch_size\", \"seq_len\"], # the ids of the prompt\n",
    "    ) -> Tuple[\n",
    "        TensorType[\"batch_size\", \"n_positions\"], # The positions of api call\n",
    "        TensorType[\"batch_size\", \"seq_len\"] # The generated text\n",
    "    ]:\n",
    "        # TODO: add support batch\n",
    "        \n",
    "        # the ids of the prompt and generated_ids\n",
    "        prompt_and_generated_ids = prompt_ids\n",
    "        # only the ids of the generated_ids\n",
    "        generated_ids = torch.tensor([])\n",
    "        api_positions = torch.tensor([])\n",
    "        i = torch.tensor([0])\n",
    "        \n",
    "        with torch.no_grad():    \n",
    "            while True:\n",
    "                logits = self.model(\n",
    "                    input_ids=prompt_and_generated_ids.unsqueeze(0),\n",
    "                ).logits\n",
    "\n",
    "                last_logit = logits[0, -1, :]\n",
    "                probs = torch.softmax(last_logit, dim=-1)\n",
    "                \n",
    "                # find the top k tokens for api call\n",
    "                # TODO: add filter by larger than sampling_threshold\n",
    "                top_k_tokens = torch.topk(probs, k=5, dim=-1)\n",
    "                \n",
    "                if self.api_start_token in top_k_tokens.indices:\n",
    "                    # api_position = torch.tensor([len(generated_ids)]) # the current idx\n",
    "                    api_positions = torch.cat([api_positions, i], dim=0)\n",
    "                \n",
    "                # sampling a token\n",
    "                # next_token = torch.multinomial(probs, num_samples=1)\n",
    "                next_token = torch.argmax(probs, dim=-1)\n",
    "                next_token = next_token.unsqueeze(0)\n",
    "                \n",
    "                prompt_and_generated_ids = torch.cat([prompt_and_generated_ids, next_token], dim=0)\n",
    "                generated_ids = torch.cat([generated_ids, next_token], dim=0)\n",
    "                \n",
    "                if next_token == self.eos_token_id:\n",
    "                    break\n",
    "                else:\n",
    "                    i += 1\n",
    "        \n",
    "        return api_positions.long(), generated_ids.long()\n",
    "\n",
    "    def obtain_api_response(\n",
    "        self,\n",
    "        prompt_ids: TensorType[\"batch_size\", \"seq_len\"],\n",
    "        positions: TensorType[\"batch_size\", \"n_positions\"],\n",
    "        generated_ids: TensorType[\"batch_size\", \"seq_len\"]\n",
    "    ) -> TensorType[\"batch_size\", \"n_positions\", \"seq_len\"]:\n",
    "        \n",
    "        MAX_PAD = 50\n",
    "        \n",
    "        # the ids before the start of an api call\n",
    "        pre_api_ids = torch.tensor([])\n",
    "\n",
    "        for position in positions:\n",
    "            text_ids = torch.cat([generated_ids[:position], self.api_start_token], dim=0)\n",
    "            padded_text_ids = F.pad(text_ids, pad=(MAX_PAD - text_ids.shape[-1], 0), value=self.pad_token_id)\n",
    "            \n",
    "            pre_api_ids = torch.cat([\n",
    "                pre_api_ids,\n",
    "                rearrange(padded_text_ids, \"... -> 1 ...\")\n",
    "            ])\n",
    "        \n",
    "        PROMPT_LENGTH = len(prompt_ids)\n",
    "        \n",
    "        # TODO: optimzie this\n",
    "        prompt_and_pre_api_ids = torch.tensor([])\n",
    "        for x in pre_api_ids:\n",
    "            prompt_and_pre_api_ids = torch.cat([\n",
    "                prompt_and_pre_api_ids,\n",
    "                torch.cat([prompt_ids, x]).unsqueeze(0)\n",
    "            ], dim=0)\n",
    "                     \n",
    "        with torch.no_grad():\n",
    "            candidates = self.model.generate(\n",
    "                input_ids=prompt_and_pre_api_ids.long(),\n",
    "                eos_token_id=self.eos_token_id,\n",
    "                max_new_tokens=50,\n",
    "            )\n",
    "        \n",
    "        # filter out the prompt template\n",
    "        # only keep the generated ids\n",
    "        candidates = candidates[:, PROMPT_LENGTH:]\n",
    "        \n",
    "        return candidates\n",
    "    \n",
    "    def _generate_conditioning_prompts(\n",
    "        self,\n",
    "        candidate_ids: TensorType[\"batch_size\", \"n_candidates\", \"seq_len\"],\n",
    "    ):\n",
    "        calculator_api = CalculatorAPI()\n",
    "        conditioning_api_ids = torch.tensor([])\n",
    "\n",
    "        API_NAME = \"Calculator\"\n",
    "        MAX_PAD = 100\n",
    "\n",
    "        for text_ids in candidate_ids:\n",
    "            # the ids of the prediction\n",
    "            text = self.tokenizer.decode(text_ids, skip_special_tokens=True)\n",
    "            \n",
    "            api_request_content = self.extract_api_request_content(text, api_name=API_NAME)\n",
    "            api_response = calculator_api(api_request_content)\n",
    "            api_response_ids = self.tokenizer(api_response, return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "            # Format: \"-> [api_response]\"\n",
    "            api_response_with_arrow_ids = torch.cat([self.api_output_token, api_response_ids], dim=0)\n",
    "            \n",
    "            api_syntax = self.extract_api_syntax(text, api_name=API_NAME)\n",
    "            api_syntax_ids = self.tokenizer(api_syntax, return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "            api_syntax_with_response_ids = torch.cat([api_syntax_ids[:-1], api_response_with_arrow_ids, api_syntax_ids[-1:]])\n",
    "            api_syntax_without_response_ids = torch.cat([api_syntax_ids[:-1], self.api_output_token, api_syntax_ids[-1:]])\n",
    "                              \n",
    "            padded_api_without_response = rearrange(\n",
    "                F.pad(api_syntax_without_response_ids, pad=((MAX_PAD - api_syntax_without_response_ids.shape[-1]), 0), value=self.pad_token_id),\n",
    "                \"... -> 1 ...\"\n",
    "            )\n",
    "            padded_api_with_response = rearrange(\n",
    "                F.pad(api_syntax_with_response_ids, pad=((MAX_PAD - api_syntax_with_response_ids.shape[-1]), 0), value=self.pad_token_id),\n",
    "                \"... -> 1 ...\"\n",
    "            )\n",
    "        \n",
    "            padded_api_call = torch.cat([\n",
    "                padded_api_without_response,\n",
    "                padded_api_with_response\n",
    "            ], dim=0)\n",
    "            padded_api_call = rearrange(padded_api_call, \"... -> 1 ...\")\n",
    "            \n",
    "            conditioning_api_ids = torch.cat([conditioning_api_ids, padded_api_call], dim=0).long()\n",
    "                    \n",
    "        return conditioning_api_ids\n",
    "\n",
    "    def _compute_weight(self, t: int) -> Union[int, float]:\n",
    "        \"\"\"Compute the weight in the loss function.\"\"\"\n",
    "        return max(0, 1-0.2*t)\n",
    "\n",
    "    \n",
    "    def _normalize_weights(self, augmented_text_ids):\n",
    "        \"\"\"Normalize the weight of each position in a sequence.\"\"\"\n",
    "        for api_start_position in augmented_text_ids[\"api_start_positions\"].values():\n",
    "            total_weight = sum([seq_position[\"unnormalized_weight\"] for seq_position in api_start_position[\"seq_positions\"].values()])\n",
    "            for seq_position in api_start_position[\"seq_positions\"].values():\n",
    "                seq_position[\"normalized_weight\"] = seq_position[\"unnormalized_weight\"] / total_weight\n",
    "        \n",
    "        return augmented_text_ids\n",
    "    \n",
    "    def _calculate_weighted_loss(self, augmented_text_ids):\n",
    "        for position in augmented_text_ids[\"api_start_positions\"]:        \n",
    "            seq_positions = augmented_text_ids[\"api_start_positions\"][position][\"seq_positions\"]\n",
    "            for i in seq_positions:\n",
    "                losses = seq_positions[i][\"losses\"]\n",
    "                weights = seq_positions[i][\"normalized_weight\"]\n",
    "                seq_positions[i][\"weighted_losses\"] = -losses * weights\n",
    "        \n",
    "        return augmented_text_ids\n",
    "    \n",
    "    def _calculate_loss(self, augmented_text_ids):\n",
    "        losses = {}\n",
    "        for position in augmented_text_ids[\"api_start_positions\"]:        \n",
    "            seq_positions = augmented_text_ids[\"api_start_positions\"][position][\"seq_positions\"]\n",
    "            three_loss = [0, 0, 0]            \n",
    "            for i in seq_positions:\n",
    "                three_loss[0] += seq_positions[i][\"weighted_losses\"][0]\n",
    "                three_loss[1] += seq_positions[i][\"weighted_losses\"][1]\n",
    "                three_loss[2] += seq_positions[i][\"weighted_losses\"][2]\n",
    "            losses[position] = three_loss\n",
    "            \n",
    "        return losses\n",
    "\n",
    "    def filter_api_candidate_by_threshold(self, losses, candidates):\n",
    "        filtered_augmented_text_ids = []\n",
    "        for i, position in enumerate(losses):\n",
    "            negative_loss = min(losses[position][0], losses[position][1])\n",
    "            positive_loss = losses[position][2]\n",
    "            \n",
    "            if negative_loss - positive_loss >= self.filtering_threshold:\n",
    "                filtered_augmented_text_ids.append(candidates[i])\n",
    "        \n",
    "        return filtered_augmented_text_ids\n",
    "    \n",
    "    def _convert_prompt_dict_to_list_input_ids(self, augmented_text_ids):\n",
    "        input_ids = []\n",
    "        for _, api_start_position_dict in augmented_text_ids[\"api_start_positions\"].items():\n",
    "            for _, seq_position_dict in api_start_position_dict[\"seq_positions\"].items():\n",
    "                for x in seq_position_dict[\"prompt_ids\"]:\n",
    "                    input_ids.append(x)\n",
    "        return input_ids\n",
    "    \n",
    "    def _convert_prompt_dict_to_list_target_ids(self, augmented_text_ids):\n",
    "        target_ids = []\n",
    "        for _, api_start_position_dict in augmented_text_ids[\"api_start_positions\"].items():\n",
    "            for _, seq_position_dict in api_start_position_dict[\"seq_positions\"].items():\n",
    "                target_ids.append(seq_position_dict[\"target_ids\"])\n",
    "        return target_ids\n",
    "\n",
    "    def filter_api( \n",
    "        self,\n",
    "        text_ids,\n",
    "        api_start_idxs,\n",
    "        candidates: TensorType[\"batch_size\", \"n_positions\", \"seq_len\"]\n",
    "    ):\n",
    "        conditioning_api_ids = self._generate_conditioning_prompts(candidates)\n",
    "                \n",
    "        SPACE_TOKEN = self.tokenizer(\". \", return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "        API_LENGTH = 100\n",
    "        augmented_text_ids = {\"api_start_positions\": {}}\n",
    "        \n",
    "        for idx, api_ids in zip(api_start_idxs, conditioning_api_ids):\n",
    "            idx = idx.item()\n",
    "            seq_len = len(text_ids)\n",
    "            augmented_text_ids[\"api_start_positions\"][idx] = {\n",
    "                \"seq_positions\": {}\n",
    "            }\n",
    "\n",
    "            j = idx\n",
    "            while j <= seq_len - 1:\n",
    "                # if the model predic\n",
    "                if j == 1:\n",
    "                    j += 1\n",
    "                    continue\n",
    "                \n",
    "                # in the formua, from x_1 to x_j (include x_j)\n",
    "                # => generate_ids[:j]\n",
    "                conditioning_text_ids = text_ids[:j]\n",
    "                api_and_text_ids = torch.stack([\n",
    "                    F.pad(conditioning_text_ids, pad=(API_LENGTH + len(SPACE_TOKEN), 0), value=self.pad_token_id), # [text_ids]\n",
    "                    torch.cat([api_ids[0], SPACE_TOKEN, conditioning_text_ids], dim=0), # [api->, text_ids]\n",
    "                    torch.cat([api_ids[1], SPACE_TOKEN, conditioning_text_ids], dim=0), # [api->result, text_ids]\n",
    "                ], dim=0)\n",
    "                                \n",
    "                # api_and_text_ids = conditioning_api_ids[]\n",
    "                # the next token after x_j\n",
    "                next_token_ids = text_ids[j]\n",
    "                augmented_text_ids[\"api_start_positions\"][idx][\"seq_positions\"][j] = {\n",
    "                    \"prompt_ids\": api_and_text_ids,\n",
    "                    \"unnormalized_weight\": self._compute_weight(t=j-idx),\n",
    "                    \"losses\": [],\n",
    "                    \"target_ids\": next_token_ids\n",
    "                }\n",
    "                j += 1\n",
    "        \n",
    "        augmented_text_ids = self._normalize_weights(augmented_text_ids)\n",
    "        input_ids = self._convert_prompt_dict_to_list_input_ids(augmented_text_ids)\n",
    "        target_ids = self._convert_prompt_dict_to_list_target_ids(augmented_text_ids)\n",
    "\n",
    "        padded_input_ids = torch.tensor([])\n",
    "        for x in input_ids:\n",
    "            padded_input_ids = torch.cat([\n",
    "                padded_input_ids,\n",
    "                F.pad(x.long(), pad=(50-x.shape[-1], 0), value=self.pad_token_id).unsqueeze(0)\n",
    "            ], dim=0)\n",
    "            \n",
    "        output = self.model(input_ids=padded_input_ids.long())\n",
    "        logits = output.logits[:, -1, :]\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        \n",
    "        log_probs = torch.tensor([])\n",
    "\n",
    "        i = 0\n",
    "        for x in target_ids:\n",
    "            log_probs = torch.cat([log_probs, probs[i:i+3][:, x].log().unsqueeze(0)], dim=0)\n",
    "            i += 3\n",
    "            \n",
    "        for _, api_start_position_dict in augmented_text_ids[\"api_start_positions\"].items():\n",
    "            for _, seq_position_dict in api_start_position_dict[\"seq_positions\"].items():\n",
    "                seq_position_dict[\"losses\"] = log_probs[:1].squeeze(0)\n",
    "                log_probs = log_probs[1:]\n",
    "        \n",
    "        augmented_text_ids = self._calculate_weighted_loss(augmented_text_ids)\n",
    "        losses = self._calculate_loss(augmented_text_ids)\n",
    "        filtered_candidate_ids = self.filter_api_candidate_by_threshold(losses, candidates)\n",
    "        \n",
    "        return filtered_candidate_ids\n",
    "    \n",
    "    def generate(\n",
    "        self,\n",
    "        prompt_tempalte: PromptTemplate,\n",
    "        text: str,\n",
    "    ) -> TensorType[\"batch_size\", \"seq_len\"]:\n",
    "        # TODO: add support batch\n",
    "        prompt = prompt_tempalte.format(input=text)\n",
    "        prompt_ids = self.tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "    \n",
    "        # sampling positions\n",
    "        api_start_idxs, generated_ids = self.sample_api_position(prompt_ids)\n",
    "        \n",
    "        # obtaining api responses\n",
    "        candidates = self.obtain_api_response(prompt_ids, api_start_idxs, generated_ids)\n",
    "\n",
    "        # filtering\n",
    "        text_ids = self.tokenizer(text, return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "        filtered_candidate_ids = self.filter_api(text_ids, api_start_idxs, candidates)\n",
    "                \n",
    "        return filtered_candidate_ids"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
