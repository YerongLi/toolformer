{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Inference\n",
    "\n",
    "> Fill in a module description hered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/education/DATA/projects/ai/toolformer/env/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "from typing import Optional, List\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from torchtyping import TensorType\n",
    "\n",
    "from toolformer.api import BaseAPI\n",
    "from toolformer.utils import extract_api_request_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect\n",
    "# wait for end of token\n",
    "# extract\n",
    "# excute\n",
    "# add the result to the input\n",
    "# continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ToolFormer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: AutoModelForCausalLM,\n",
    "        apis: List[BaseAPI],\n",
    "        config: dict\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.apis = apis\n",
    "        self.config = config\n",
    "        self.is_calling_api: bool = False\n",
    "        \n",
    "        # TODO: make a config class contains token_id\n",
    "        tokenizer = AutoTokenizer.from_pretrained(self.config[\"tokenizer\"][\"path\"])\n",
    "        self.tokenizer = tokenizer # TODO: remove after debug\n",
    "        \n",
    "        start_character = config[\"data_generator\"][\"api_start_character\"]\n",
    "        end_character = config[\"data_generator\"][\"api_end_character\"]\n",
    "        output_character = config[\"data_generator\"][\"api_output_character\"]\n",
    "        \n",
    "        self.api_start_token_id = tokenizer(f' {start_character}', return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "        self.api_end_token_id = tokenizer(end_character, return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "        self.api_output_token_id = tokenizer(f'{output_character}', return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "    \n",
    "        # TODO: support batch\n",
    "        self.api_request_content: torch.Tensor = torch.tensor([])\n",
    "    \n",
    "    def _sampling(self, probs: TensorType[\"batch_size\", \"seq_len\"]) -> TensorType[\"batch_size\", \"seq_len\"]:\n",
    "        return torch.argmax(probs, dim=-1)\n",
    "    \n",
    "    def execute_api(self, text_ids: TensorType[\"seq_len\"]) -> TensorType[\"seq_len\"]:\n",
    "        \"\"\"Execute an API call.\"\"\"\n",
    "        # content_ids = extract_api_request_content(text_ids, self.apis)\n",
    "        pass\n",
    "    \n",
    "    def add_idx_to_api_request_content(self, idx: TensorType[1]):\n",
    "        self.api_request_content = torch.cat([self.api_request_content, idx.unsqueeze(0)], dim=0)\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: TensorType[\"batch_size\", \"seq_len\"],\n",
    "        attention_mask: Optional[TensorType[\"batch_size\", \"seq_len\"]]=None,\n",
    "        max_new_tokens: int = 10,\n",
    "        **kwargs\n",
    "    ) -> TensorType[\"batch_size\", \"seq_len\"]:\n",
    "        # check padding to the left\n",
    "        \n",
    "        generated_ids = input_ids\n",
    "        \n",
    "        for _ in range(max_new_tokens):\n",
    "            output_ids = self.model(\n",
    "                input_ids=generated_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                **kwargs\n",
    "            )\n",
    "            \n",
    "            logits = output_ids.logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            _, top_k_idx = torch.topk(probs, k=5, dim=-1)\n",
    "            \n",
    "            if self.is_calling_api is True:\n",
    "                if self.api_end_token_id in top_k_idx:\n",
    "                    # if the api end token is in the top_k_idx, then we will execute the api\n",
    "                    # and then add api_end_token_id to the generated_ids\n",
    "                    self.add_idx_to_api_request_content(self.api_end_token_id)\n",
    "                    api_output_ids = self.execute_api(self.api_request_content)\n",
    "                    pred_ids = torch.tensor([self.api_end_token_id, api_output_ids])\n",
    "                    self.is_calling_api = False\n",
    "                else:\n",
    "                    pred_ids = self._sampling(probs)\n",
    "                    self.add_idx_to_api_request_content(pred_ids)\n",
    "            else:\n",
    "                if self.api_start_token_id in top_k_idx:\n",
    "                    # if the api start token is in the top_k_idx, then we are calling an api\n",
    "                    self.is_calling_api = True\n",
    "                    pred_ids = self.api_start_token_id\n",
    "                    self.add_idx_to_api_request_content(pred_ids)\n",
    "                else:\n",
    "                    pred_ids = self._sampling(probs)\n",
    "            \n",
    "            generated_ids = torch.cat([generated_ids, pred_ids.unsqueeze(dim=1)], dim=1)\n",
    "            attention_mask = torch.cat([attention_mask, torch.ones_like(pred_ids).unsqueeze(dim=1)], dim=1)\n",
    "        \n",
    "        return generated_ids"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
