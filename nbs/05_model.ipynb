{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Inference\n",
    "\n",
    "> Fill in a module description hered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/education/DATA/projects/ai/toolformer/env/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "from typing import Optional, List\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from torchtyping import TensorType\n",
    "\n",
    "from toolformer.api import BaseAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect\n",
    "# wait for end of token\n",
    "# extract\n",
    "# excute\n",
    "# add the result to the input\n",
    "# continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ToolFormer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: AutoModelForCausalLM,\n",
    "        apis: List[BaseAPI],\n",
    "        config: dict\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.apis = apis\n",
    "        self.config = config\n",
    "        self.is_calling_api: bool = False\n",
    "        \n",
    "        # TODO: make a config class contains token_id\n",
    "        tokenizer = AutoTokenizer.from_pretrained(self.config[\"tokenizer\"][\"path\"])\n",
    "        \n",
    "        start_character = config[\"data_generator\"][\"api_start_character\"]\n",
    "        end_character = config[\"data_generator\"][\"api_end_character\"]\n",
    "        output_character = config[\"data_generator\"][\"api_output_character\"]\n",
    "        \n",
    "        self.api_start_token_id = tokenizer(f' {start_character}', return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "        self.api_end_token_id = tokenizer(end_character, return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "        self.api_output_token_id = tokenizer(f'{output_character}', return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "    \n",
    "    def sampling(self, probs: TensorType[\"batch_size\", \"seq_len\"]) -> TensorType[1]:\n",
    "        return torch.argmax(probs, dim=-1)\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: TensorType[\"batch_size\", \"seq_len\"],\n",
    "        attention_mask: Optional[TensorType[\"batch_size\", \"seq_len\"]]=None,\n",
    "        max_new_tokens: int = 10,\n",
    "        **kwargs\n",
    "    ) -> TensorType[\"batch_size\", \"seq_len\"]:\n",
    "                \n",
    "        generated_ids = input_ids\n",
    "        for _ in range(max_new_tokens):\n",
    "            output = self.model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                **kwargs\n",
    "            )\n",
    "            \n",
    "            logits = output.logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            _, top_k_idx = torch.topk(probs, k=5, dim=-1)\n",
    "            \n",
    "            if self.is_calling_api is True:\n",
    "                if self.api_end_token_id in top_k_idx:\n",
    "                    self.is_calling_api = False\n",
    "                    pred_idx = self.api_end_token_id\n",
    "                else:\n",
    "                    pred_idx = self.sampling(probs)\n",
    "            else:\n",
    "                if self.api_start_token_id in top_k_idx:\n",
    "                    self.is_calling_api = True\n",
    "                    pred_idx = self.api_start_token_id\n",
    "                else:\n",
    "                    pred_idx = self.sampling(probs)\n",
    "            \n",
    "            generated_ids = torch.cat([generated_ids, pred_idx.unsqueeze(0)], dim=-1)\n",
    "        \n",
    "        return generated_ids"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
